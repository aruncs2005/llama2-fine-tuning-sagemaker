{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Train LLama2-7B model with multi-node clusters on Amazon SageMaker using Hugging Face and PyTorch FSDP\n",
    "\n",
    "In this tutorial, we will fine-tune the new [LLama2-7B](https://huggingface.co/meta-llama/Llama-2-7b) on the [Alpaca](https://huggingface.co/datasets/tatsu-lab/alpaca) dataset to improve the question-answering skills on code.\n",
    "\n",
    "[LLama2-7B](https://huggingface.co/meta-llama/Llama-2-7b) is a 7B open-source LLM, which makes it hard to fine-tune on a single GPU or even a single Node with multiple GPUs. We are going to use Amazon SageMaker managed training platform as our infrastructure backbone to help us create a multi-node cluster to easily run our distributed training. As instances, we will use 2x p4d.24xlarge instances, which come with 8x NIVIDA A100 40GB GPUs. \n",
    "\n",
    "*Note: For the purpose of this workshop we will use a smaller 7 billion Parameter model and will use G5.2xlarge instance which comes with 1 Nvidia A10G 24GB GPU.*\n",
    "\n",
    "For training framework we will use Hugging Face Transformers Trainer + PEFT LORA 8 bit training, which will make it super easy to train model without worrying about distribution mechanism.\n",
    "\n",
    "\n",
    "## What is Low Rank Adaptation (LORA)?\n",
    "\n",
    "LORA is a mechanism that helps in Efficient Fine tuning of LLM's.To make fine-tuning more efficient, LoRA’s approach is to represent the weight updates with two smaller matrices (called update matrices) through low-rank decomposition. These new matrices can be trained to adapt to the new data while keeping the overall number of changes low. The original weight matrix remains frozen and doesn’t receive any further adjustments. To produce the final results, both the original and the adapted weights are combined.\n",
    "\n",
    "This approach has a number of advantages::\n",
    "\n",
    "- LoRA makes fine-tuning more efficient by drastically reducing the number of trainable parameters.\n",
    "- The original pre-trained weights are kept frozen, which means you can have multiple lightweight and portable LoRA models for various downstream tasks built on top of them.\n",
    "- LoRA is orthogonal to many other parameter-efficient methods and can be combined with many of them.\n",
    "- Performance of models fine-tuned using LoRA is comparable to the performance of fully fine-tuned models.\n",
    "- LoRA does not add any inference latency because adapter weights can be merged with the base model.\n",
    "\n",
    "\n",
    "PEFT LORA is natively integrated into the [Hugging Face Trainer](https://huggingface.co/docs/transformers/main_classes/trainer#pytorch-fully-sharded-data-parallel), making it easy to adapt and use. You can learn more about PEFT in [State-of-the-art Parameter-Efficient Fine-Tuning (PEFT) methods](https://github.com/huggingface/peft)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"transformers\" \"datasets[s3]\" \"sagemaker\" \"boto3\" --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r scripts/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are going to use Sagemaker in a local environment. You need access to an IAM Role with the required permissions for Sagemaker. You can find [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) more about it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and prepare the dataset\n",
    "\n",
    "As the base dataset, we will use the [Alpaca](https://huggingface.co/datasets/tatsu-lab/alpaca) dataset, but before fine-tuning the model, we need to preprocess the data. We will create chunks of `2048` tokens ([model max length](https://huggingface.co/EleutherAI/gpt-neox-20b)) to avoid unnecessary padding and computing. \n",
    "\n",
    "The first step is to load our dataset from Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "access_token = \"hf_XXXXXXXX\"\n",
    "model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "dataset_name = \"tatsu-lab/alpaca\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer \n",
    "\n",
    "from huggingface_hub.hf_api import HfFolder;\n",
    "HfFolder.save_token(access_token)\n",
    "\n",
    "# Load Tokenizer \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id,token=access_token)\n",
    "\n",
    "# Load dataset from huggingface.co\n",
    "dataset = load_dataset(dataset_name)\n",
    "\n",
    "# downsample dataset to 10k\n",
    "dataset = dataset.shuffle(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"validation\" not in dataset.keys():\n",
    "    dataset[\"validation\"] = load_dataset(\n",
    "        dataset_name,\n",
    "        split=\"train[:5%]\"\n",
    "    )\n",
    "\n",
    "    dataset[\"train\"] = load_dataset(\n",
    "        dataset_name,\n",
    "        split=\"train[5%:]\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step of the data preparation is to tokenize and chunk our dataset. We convert our inputs (text) to token IDs by tokenizing, which the model can understand. Additionally, we concatenate our dataset samples into chunks of `2048` to avoid unnecessary padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from itertools import chain\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "def group_texts(examples,block_size = 2048):\n",
    "        # Concatenate all texts.\n",
    "        concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "        total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "        # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "        # customize this part to your needs.\n",
    "        if total_length >= block_size:\n",
    "            total_length = (total_length // block_size) * block_size\n",
    "        # Split by chunks of max_len.\n",
    "        result = {\n",
    "            k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "            for k, t in concatenated_examples.items()\n",
    "        }\n",
    "        result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "        return result\n",
    "\n",
    "column_names = dataset[\"train\"].column_names\n",
    "\n",
    "lm_dataset = dataset.map(\n",
    "    lambda sample: tokenizer(sample[\"text\"],return_token_type_ids=False), batched=True, remove_columns=list(column_names)\n",
    ").map(\n",
    "    partial(group_texts, block_size=2048),\n",
    "    batched=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fine-tune Llama V2 model using Transformers + LORA using local GPU\n",
    "\n",
    "We will use the 4 GPU's available in this notebook instance to launch a distributed training job using torch distributed(torchrun). \n",
    "\n",
    "We will start by saving the tokenized data locally ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save data locally\n",
    "\n",
    "training_input_path = f'processed/data/'\n",
    "lm_dataset.save_to_disk(training_input_path)\n",
    "\n",
    "print(f\"Saved data to: {training_input_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start the training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! torchrun --nnodes 1 \\\n",
    "        --nproc_per_node 4 \\\n",
    "        --master_addr localhost \\\n",
    "        --master_port 7777 \\\n",
    "        scripts/run_clm_lora.py \\\n",
    "        --bf16 True \\\n",
    "        --dataset_path processed/data \\\n",
    "        --output_dir model \\\n",
    "        --epochs 3 \\s\n",
    "        --fsdp \"full_shard auto_wrap\" \\\n",
    "        --fsdp_transformer_layer_cls_to_wrap LlamaDecoderLayer \\\n",
    "        --gradient_checkpointing True \\\n",
    "        --model_id meta-llama/Llama-2-7b-chat-hf \\\n",
    "        --optimizer adamw_torch \\\n",
    "        --per_device_train_batch_size 1 \\\n",
    "        --access_token {access_token} \\\n",
    "        --max_steps 100 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the training finishes the models will be saved in the model directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fine-tune the Llama2 model using Transformers + LORA on Amazon SageMaker\n",
    "\n",
    "As mentioned in the beginning, we will use Amazon SageMaker and PyTorch FSDP to train our model. Amazon SageMaker makes it easy to create a multi-node cluster to train our model in a distributed manner. The `sagemaker` python SDK supports to run training jobs using `torchrun`, to distribute the script across multiple nodes and GPUs. \n",
    "\n",
    "To use `torchrun` to execute our scripts, we only have to define the `distribution` parameter in our Estimator and set it to `\"torch_distributed\": {\"enabled\": True}`. This tells sagemaker to launch our training job with.\n",
    "\n",
    "```python\n",
    "torchrun --nnodes 2 --nproc_per_node 8 --master_addr algo-1 --master_port 7777 --node_rank 1 run_clm.py --bf16 True --dataset_path /opt/ml/input/data/training --epochs 3 --fsdp \"full_shard auto_wrap\" --fsdp_transformer_layer_cls_to_wrap LlamaDecoderLayer --gradient_checkpointing True --model_id meta-llama/Llama-2-7b-chat-hf --optimizer adamw_torch --per_device_train_batch_size 1\n",
    "```\n",
    "\n",
    "We will use the PEFT library function to create a LORA model and use int 8 for training. \n",
    "\n",
    "We prepared a run_clm_lora.py, which implements causal language modeling and accepts LORA parameters.\n",
    "\n",
    "To create a sagemaker training job, we create an `HuggingFace` Estimator and provide all our information. SagMaker takes care of starting and managing all the required ec2 instances for us, provides the correct huggingface container, uploads the provided scripts and downloads the data from our S3 bucket into the container at `/opt/ml/input/data`. Then, it starts the training job by running."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we processed the datasets we are going to use the new [FileSystem integration](https://huggingface.co/docs/datasets/filesystems) to upload our dataset to S3. We are using the `sess.default_bucket()`, adjust this if you want to store the dataset in a different S3 bucket. We will use the S3 path later in our training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload data to s3 \n",
    "\n",
    "training_input_path = f's3://{sess.default_bucket()}/processed/data/'\n",
    "print(f\"training dataset to: {training_input_path}\")# save train_dataset to s3\n",
    "lm_dataset.save_to_disk(training_input_path)\n",
    "\n",
    "print(f\"uploaded data to: {training_input_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "from sagemaker.pytorch import PyTorch\n",
    "# define Training Job Name \n",
    "job_name = f'huggingface-fsdp-{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())}'\n",
    "\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters={\n",
    "    'model_id': model_id, # model id from huggingface.co/models\n",
    "    'dataset_path': '/opt/ml/input/data/train', # path where sagemaker will save training dataset\n",
    "    'valid_path':\"/opt/ml/input/data/valid\",\n",
    "    'gradient_checkpointing': True, # enable gradient checkpointing\n",
    "    'bf16': True, # enable mixed precision training\n",
    "    'optimizer': \"adamw_torch\", # optimizer\n",
    "    'per_device_train_batch_size': 1, # batch size per device during training\n",
    "    'epochs': 1, # number of epochs to train\n",
    "    'fsdp': '\"full_shard auto_wrap\"', # fully sharded data parallelism\n",
    "    'fsdp_transformer_layer_cls_to_wrap': \"LlamaDecoderLayer\", # transformer layer to wrap\n",
    "    'max_steps':100,\n",
    "    'access_token': access_token\n",
    "}\n",
    "\n",
    "# this environment variables are required for P4d instances to enable EFA.\n",
    "env = {}\n",
    "env['FI_PROVIDER'] = 'efa'\n",
    "env['NCCL_PROTO'] = 'simple'\n",
    "env['FI_EFA_USE_DEVICE_RDMA'] = '1'\n",
    "env['RDMAV_FORK_SAFE'] = '1'\n",
    "\n",
    "# estimator \n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point='run_clm_lora.py',\n",
    "    source_dir='./scripts',\n",
    "    instance_type=\"ml.g5.12xlarge\",\n",
    "    instance_count=1,\n",
    "    role=role,\n",
    "    job_name=job_name,\n",
    "    transformers_version='4.28.1',\n",
    "    pytorch_version='2.0.0',\n",
    "    py_version=\"py310\",\n",
    "    environment=env,\n",
    "    hyperparameters = hyperparameters,\n",
    "    disable_output_compression=True,\n",
    "    keep_alive_period_in_seconds=600,\n",
    "    distribution={\"torch_distributed\": {\"enabled\": True}} # enable torchrun \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now start our training job, with the `.fit()` method passing our S3 path to the training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a data input dictonary with our uploaded s3 uris\n",
    "data = {'train': training_input_path}\n",
    "\n",
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit(data, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terminate the warm pool cluster if no longer needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.update_training_job(huggingface_estimator.latest_training_job.job_name, resource_config={\"KeepAlivePeriodInSeconds\":0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
